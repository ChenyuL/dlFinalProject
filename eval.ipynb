{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eval.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import pickle\n",
        "import argparse\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip\n",
        "import shutil\n",
        "import spacy \n",
        "import pandas as pd \n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import kora.install.rdkit\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw, MolFromSmiles, MolToSmiles\n",
        "from sklearn.manifold import TSNE\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "import math\n",
        "import os\n",
        "from torch import optim\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "25PMxgq3Bc2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SandwichTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model = 512, nhead = 8, num_encoder_layers = 6,\n",
        "                 num_decoder_layers = 6, dim_feedforward = 2048, dropout = 0.1,\n",
        "                 sandwich_k = 2, sandwich_encoder = False, sandwich_decoder = False,\n",
        "                 activation = F.relu, layer_norm_eps = 1e-5):\n",
        "        super(SandwichTransformer, self).__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
        "                                                activation, layer_norm_eps)\n",
        "        encoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm, sandwich_k if sandwich_encoder else 0)\n",
        "\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
        "                                                activation, layer_norm_eps)\n",
        "        decoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm, sandwich_k if sandwich_decoder else 0)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "        self.sandwich_k = sandwich_k\n",
        "        self.sandwich_encoder = sandwich_encoder\n",
        "        self.sandwich_decoder = sandwich_decoder\n",
        "\n",
        "    def forward(self, src, tgt, src_mask = None, tgt_mask = None, memory_mask = None,\n",
        "                src_key_padding_mask = None, tgt_key_padding_mask = None,\n",
        "                memory_key_padding_mask = None):\n",
        "\n",
        "        is_batched = src.dim() == 3\n",
        "\n",
        "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask, )\n",
        "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
        "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                              memory_key_padding_mask=memory_key_padding_mask)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def generate_square_subsequent_mask(sz):\n",
        "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    _constants_ = ['norm']\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None, sandwich_k=0):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers+sandwich_k)\n",
        "        self.num_layers = num_layers+sandwich_k\n",
        "        self.norm = norm\n",
        "        self.sandwich_k = sandwich_k\n",
        "\n",
        "    def forward(self, src, mask = None, src_key_padding_mask = None):\n",
        "        output = src\n",
        "        for i, mod in enumerate(self.layers):\n",
        "            if i < self.sandwich_k:\n",
        "                output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, layer_type=1)\n",
        "            elif i >= self.num_layers - self.sandwich_k:\n",
        "                output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, layer_type=2)\n",
        "            else:\n",
        "                output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, layer_type=0)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    _constants_ = ['norm']\n",
        "\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None, sandwich_k=0):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers+sandwich_k)\n",
        "        self.num_layers = num_layers+sandwich_k\n",
        "        self.norm = norm\n",
        "        self.sandwich_k = sandwich_k\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask = None,\n",
        "                memory_mask = None, tgt_key_padding_mask = None,\n",
        "                memory_key_padding_mask = None):\n",
        "        output = tgt\n",
        "\n",
        "        for i, mod in enumerate(self.layers):\n",
        "            if i < self.sandwich_k:\n",
        "                output = mod(output, memory, tgt_mask=tgt_mask,\n",
        "                      memory_mask=memory_mask,\n",
        "                      tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                      memory_key_padding_mask=memory_key_padding_mask, layer_type=1)\n",
        "            elif i >= self.num_layers - self.sandwich_k:\n",
        "                output = mod(output, memory, tgt_mask=tgt_mask,\n",
        "                      memory_mask=memory_mask,\n",
        "                      tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                      memory_key_padding_mask=memory_key_padding_mask, layer_type=2)\n",
        "            else:\n",
        "                output = mod(output, memory, tgt_mask=tgt_mask,\n",
        "                      memory_mask=memory_mask,\n",
        "                      tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                      memory_key_padding_mask=memory_key_padding_mask, layer_type=0)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    _constants_ = ['batch_first', 'norm_first']\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward = 2048, dropout = 0.1,\n",
        "                 activation = F.relu, layer_norm_eps = 1e-5):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "    def _setstate_(self, state):\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super(TransformerEncoderLayer, self)._setstate_(state)\n",
        "\n",
        "    def forward(self, src, src_mask = None, src_key_padding_mask = None,\n",
        "                layer_type=0):\n",
        "        x = src\n",
        "\n",
        "        if layer_type == 0:\n",
        "            x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))\n",
        "            x = self.norm2(x + self._ff_block(x))\n",
        "        elif layer_type == 1:\n",
        "            x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))\n",
        "        else:\n",
        "            x = self.norm2(x + self._ff_block(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    # self-attention block\n",
        "    def _sa_block(self, x, attn_mask, key_padding_mask):\n",
        "        x = self.self_attn(x, x, x,\n",
        "                           attn_mask=attn_mask,\n",
        "                           key_padding_mask=key_padding_mask,\n",
        "                           need_weights=False)[0]\n",
        "        return self.dropout1(x)\n",
        "\n",
        "    # feed forward block\n",
        "    def _ff_block(self, x):\n",
        "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
        "        return self.dropout2(x)\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    _constants_ = ['batch_first', 'norm_first']\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward = 2048, dropout = 0.1,\n",
        "                 activation = F.relu, layer_norm_eps = 1e-5):\n",
        "      \n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "    def _setstate_(self, state):\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super(TransformerDecoderLayer, self)._setstate_(state)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask = None, memory_mask= None,\n",
        "                tgt_key_padding_mask = None, memory_key_padding_mask = None,\n",
        "                layer_type = 0):\n",
        "        x = tgt\n",
        "        if layer_type == 0:\n",
        "            x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
        "            x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))\n",
        "            x = self.norm3(x + self._ff_block(x))\n",
        "        elif layer_type == 1:\n",
        "            x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
        "            x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))\n",
        "        else:\n",
        "            x = self.norm3(x + self._ff_block(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    # self-attention block\n",
        "    def _sa_block(self, x, attn_mask , key_padding_mask):\n",
        "        x = self.self_attn(x, x, x,\n",
        "                           attn_mask=attn_mask,\n",
        "                           key_padding_mask=key_padding_mask,\n",
        "                           need_weights=False)[0]\n",
        "        return self.dropout1(x)\n",
        "\n",
        "    # multihead attention block\n",
        "    def _mha_block(self, x, mem, attn_mask, key_padding_mask):\n",
        "        x = self.multihead_attn(x, mem, mem,\n",
        "                                attn_mask=attn_mask,\n",
        "                                key_padding_mask=key_padding_mask,\n",
        "                                need_weights=False)[0]\n",
        "        return self.dropout2(x)\n",
        "\n",
        "    # feed forward block\n",
        "    def _ff_block(self, x):\n",
        "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
        "        return self.dropout3(x)\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return F.gelu\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout = 0.1, max_len = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "3pZZY_eub_qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD = 0\n",
        "UNK = 1\n",
        "EOS = 2\n",
        "SOS = 3\n",
        "MASK = 4\n",
        "\n",
        "class SandwichSmTrfm(nn.Module):\n",
        "    def __init__(self, in_size, hidden_size, out_size, n_layers, sandwich_k, sandwich_encoder, sandwich_decoder):\n",
        "        super(SandwichSmTrfm, self).__init__()\n",
        "        self.in_size = in_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed = nn.Embedding(in_size, hidden_size)\n",
        "        self.pe = PositionalEncoding(hidden_size, 0.1)\n",
        "        self.sandwich_k = sandwich_k\n",
        "        self.sandwich_encoder = sandwich_encoder\n",
        "        self.sandwich_decoder = sandwich_decoder\n",
        "        self.trfm = SandwichTransformer(d_model=hidden_size, nhead=4, \n",
        "        num_encoder_layers=n_layers, num_decoder_layers = n_layers, dim_feedforward=hidden_size,\n",
        "        sandwich_k=sandwich_k, sandwich_encoder=sandwich_encoder, sandwich_decoder=sandwich_decoder,\n",
        "        dropout=0.4)\n",
        "        self.out = nn.Linear(hidden_size, out_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embed(src)\n",
        "        embedded += self.pe(embedded)\n",
        "        hidden = self.trfm(embedded, embedded)\n",
        "        out = self.out(hidden)\n",
        "        out = F.log_softmax(out, dim=2)\n",
        "        return out\n",
        "\n",
        "    def _encode(self, src):\n",
        "        embedded = self.embed(src)\n",
        "        embedded = self.pe(embedded)\n",
        "        output = embedded\n",
        "        if self.sandwich_encoder:\n",
        "            for i in range(self.trfm.encoder.num_layers - 1):\n",
        "                if i < self.sandwich_k:\n",
        "                    output = self.trfm.encoder.layers[i](output, None, layer_type=1)\n",
        "                elif i >= self.trfm.encoder.num_layers - self.sandwich_k:\n",
        "                    output = self.trfm.encoder.layers[i](output, None, layer_type=2)\n",
        "                else:\n",
        "                    output = self.trfm.encoder.layers[i](output, None, layer_type=0)\n",
        "            penul = output.cpu().detach().numpy()\n",
        "            output = self.trfm.encoder.layers[-1](output, None, layer_type=2)\n",
        "        else:\n",
        "            for i in range(self.trfm.encoder.num_layers - 1):\n",
        "                output = self.trfm.encoder.layers[i](output, None, layer_type=0)\n",
        "            penul = output.cpu().detach().numpy()\n",
        "            output = self.trfm.encoder.layers[-1](output, None, layer_type=0)\n",
        "        if self.trfm.encoder.norm:\n",
        "            output = self.trfm.encoder.norm(output)\n",
        "        output = output.cpu().detach().numpy()\n",
        "        \n",
        "        return np.hstack([np.mean(output, axis=0), np.max(output, axis=0), output[0,:,:], penul[0,:,:]])\n",
        "    \n",
        "    def encode(self, src):\n",
        "        batch_size = src.shape[1]\n",
        "        if batch_size <= 100:\n",
        "            return self._encode(src)\n",
        "        else: # Batch is too large to load\n",
        "            i = 0\n",
        "            while i < batch_size:\n",
        "                if o == 0:\n",
        "                    out = self._encode(src[:, i:i+100])\n",
        "                else:\n",
        "                    out = np.concatenate([out, self._encode(src[:, i:i+100])], axis=0)\n",
        "                i += 100\n",
        "            return out\n",
        "\n",
        "def evaluate(model, test_loader, vocab):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for b, sm in enumerate(test_loader):\n",
        "        sm = torch.t(sm.cuda())\n",
        "        with torch.no_grad():\n",
        "            output = model(sm)\n",
        "        loss = F.nll_loss(output.view(-1, len(vocab)),\n",
        "                               sm.contiguous().view(-1),\n",
        "                               ignore_index=PAD)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(test_loader)\n"
      ],
      "metadata": {
        "id": "VSQmfWO8cE_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SandwichSmTrfm(45, 256, 45, 4, 1, False, True)"
      ],
      "metadata": {
        "id": "XbMGVMgwcKzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('/content/drive/MyDrive/project_best.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "id": "GF9IQbEDBYnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZzzS1icBNhN"
      },
      "outputs": [],
      "source": [
        "def eval_mlp(X, y, rate, n_repeats):\n",
        "    auc = np.empty(n_repeats)\n",
        "    for i in range(n_repeats):\n",
        "        clf = MLPClassifier(max_iter=1000)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-rate, stratify=y)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_score = clf.predict_proba(X_test)\n",
        "        auc[i] = roc_auc_score(y_test, y_score[:,1])\n",
        "    res = {}\n",
        "    res['auc mean'] = np.mean(auc)\n",
        "    res['auc std'] = np.mean(np.std(auc, axis=0))\n",
        "    return res\n",
        "\n",
        "def get_inputs(sm):\n",
        "    seq_len = 220\n",
        "    sm = sm.split()\n",
        "    if len(sm) > 218:\n",
        "        sm = sm[:109]+sm[-109:]\n",
        "    ids = [vocab.word_dict.get(token, unk_index) for token in sm]\n",
        "    ids = [sos_index] + ids + [eos_index]\n",
        "    seg = [1] * len(ids)\n",
        "    padding = [pad_index] * (seq_len - len(ids))\n",
        "    ids.extend(padding)\n",
        "    seg.extend(padding)\n",
        "    return ids, seg\n",
        "\n",
        "def get_array(smiles):\n",
        "    x_id, x_seg = [], []\n",
        "    for sm in smiles:\n",
        "        a,b = get_inputs(sm)\n",
        "        x_id.append(a)\n",
        "        x_seg.append(b)\n",
        "    return torch.tensor(x_id), torch.tensor(x_seg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fs = [('/content/drive/MyDrive/BBBP.csv', 'p_np', 'smiles'), ('/content/drive/MyDrive/HIV.csv', 'HIV_active', 'smiles'), ('/content/drive/MyDrive/bace.csv', 'Class', 'mol')]\n",
        "for f in fs:\n",
        "    model.eval()\n",
        "    pad_index = 0\n",
        "    unk_index = 1\n",
        "    eos_index = 2\n",
        "    sos_index = 3\n",
        "    mask_index = 4\n",
        "    df = pd.read_csv(f[0])\n",
        "    rates = 2**np.arange(7)/80\n",
        "    x_split = [split(sm) for sm in df[f[2]].values]\n",
        "    xid, _ = get_array(x_split)\n",
        "    X = model.encode(torch.t(xid).cuda())\n",
        "    mean_score = np.zeros(len(rates))\n",
        "    print(X.shape)\n",
        "    for i, rate in enumerate(rates):\n",
        "        score_dic = eval_mlp(X, df[f[1]].values, rate, 20)\n",
        "        mean_score[i] = score_dic['auc mean']\n",
        "        print(rate, score_dic)\n",
        "    \n",
        "    print(np.mean(mean_score))"
      ],
      "metadata": {
        "id": "6YmtM0VIBROc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "X = model.encode(torch.t(xid).cuda())"
      ],
      "metadata": {
        "id": "YDelsnWzEkqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_reduced = TSNE(n_components=2, random_state=0).fit_transform(X)"
      ],
      "metadata": {
        "id": "JgCJGPvkEoFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8,7))\n",
        "plt.rcParams['font.size'] = 14\n",
        "plt.rcParams['font.size'] = 12\n",
        "# p_np for bbbp, Class for bace, 'HIV_active' for hiv and use df1 not df\n",
        "plt.scatter(X_reduced[df['HIV_active']==0][:,0], X_reduced[df['HIV_active']==0][:,1], label='negative', marker='o',alpha=0.5)\n",
        "plt.scatter(X_reduced[df['HIV_active']==1][:,0], X_reduced[df['HIV_active']==1][:,1], label='positive', marker='o', alpha=0.8)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.legend(loc='upper left')\n",
        "plt.savefig('/content/11785FinalProject/LatentSpaceImages/HIV/HIV_baseline.png')"
      ],
      "metadata": {
        "id": "LyY4z_DiErQC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}